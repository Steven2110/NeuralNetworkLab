# -*- coding: utf-8 -*-
"""Lab1_Wijaya.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1b299CYVwCacH3JuOQp9DxEbuC3bZ8du9
"""

from google.colab import drive
drive.mount('/content/gdrive')

!pip uninstall scikit-learn -y

!pip install -U scikit-learn

import sklearn
sklearn.__version__

import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from tensorflow.keras.utils import to_categorical
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelBinarizer, LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, RocCurveDisplay, mean_squared_error, mean_absolute_error, r2_score

"""# Загружаем все файлы с данными"""

file1_name = "/content/gdrive/MyDrive/TSU/Lab_Neural_Network/Lab1/bank.csv"
file2_name = "/content/gdrive/MyDrive/TSU/Lab_Neural_Network/Lab1/fetal_health.csv"
file3_name = "/content/gdrive/MyDrive/TSU/Lab_Neural_Network/Lab1/DS_2019.csv"

"""# Бинарный классификатор"""

data1 = pd.read_csv(file1_name, sep=';')
data1.head(10)

data1.describe()

data1.info()

"""## Transform every categorical data to numerical

### Job
"""

data1['job'].unique()

data1['job'] = data1['job'].replace({'admin.': 'admin'})
data1['job'].unique()

job_dict = {
    'unemployed': -1,
    'services': 1,
    'management': 2,
    'blue-collar': 3,
    'self-employed': 4,
    'technician': 5,
    'entrepreneur': 6,
    'admin': 7,
    'student': 8,
    'housemaid': 9,
    'retired': 10,
    'unknown': 0
}
data1['job'] = data1['job'].map(job_dict)

data1['job'].head()

"""### Marital"""

data1['marital'].unique()

marital_dict = {'married': 0, 'single': 1, 'divorced': 2}
data1['marital'] = data1['marital'].map(marital_dict)

data1['marital'].head()

"""### Education"""

data1['education'].unique()

education_dict = {'primary': 1, 'secondary': 2, 'tertiary': 3, 'unknown': 0}
data1['education'] = data1['education'].map(education_dict)

data1['education'].head()

"""### Default (has credit or not)"""

data1['default'].unique()

default_dict = {'no': 0, 'yes': 1}
data1['default'] = data1['default'].map(default_dict)

data1['default'].head()

"""### Housing"""

data1['housing'].unique()

housing_dict = {'yes': 1, 'no': 0}
data1['housing'] = data1['housing'].map(housing_dict)

data1['housing'].head()

"""### Loan"""

data1['loan'].unique()

loan_dict = {'no': 0, 'yes': 1}
data1['loan'] = data1['loan'].map(loan_dict)

data1['loan'].head()

"""### Contact"""

data1['contact'].unique()

contact_dict = {'unknown': 0, 'cellular': 1, 'telephone': 2}
data1['contact'] = data1['contact'].map(contact_dict)

data1['contact'].head()

"""### Month"""

data1['month'].unique()

month_dict = {
    'jan': 1,
    'feb': 2,
    'mar': 3,
    'apr': 4,
    'may': 5,
    'jun': 6,
    'jul': 7,
    'aug': 8,
    'sep': 9,
    'oct': 10,
    'nov': 11,
    'dec': 12
}

data1['month'] = data1['month'].map(month_dict)

data1['month'].head()

"""### Poutcome"""

data1['poutcome'].unique()

poutcome_dict = {'unknown': 0, 'success': 1, 'failure': 2, 'other': 3}
data1['poutcome'] = data1['poutcome'].map(poutcome_dict)

data1['poutcome'].head()

"""## Add Target based on attribute y"""

binary_class = {'no':0,'yes':1}
data1['Target'] = data1['y'].map(binary_class)

data1[['y', 'Target']].head(15)

data1['Target'].value_counts()

"""## Divide data into X and y"""

X = data1.drop(['y', 'Target'], axis=1)
y = data1['Target']

"""## Split data to train, test, and validation"""

X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.25, random_state=0)

X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify=y_train, test_size = 0.2, random_state = 0)

"""## Normalize data"""

ss_train = StandardScaler()
X_train = ss_train.fit_transform(X_train)

ss_test = StandardScaler()
X_test = ss_test.fit_transform(X_test)

"""## Train Model"""

model = LogisticRegression(C=0.02)
model.fit(X_train, y_train)

y_prediction = model.predict(X_test)

recall = recall_score(y_test, y_prediction, average='weighted')
precision = precision_score(y_test, y_prediction, average='weighted')
weighted_accuracy = accuracy_score(y_test, y_prediction)
auc = roc_auc_score(y_test, y_prediction)

print(f"Recall: {recall:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Weighted Accuracy: {weighted_accuracy:.2f}")
print(f"AUC: {auc:.2f}")

"""## TensorFlow"""

binary_model = tf.keras.Sequential([
    tf.keras.layers.Dense(5,activation='relu',input_dim=16),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

binary_model.summary()

binary_model.compile(loss='binary_crossentropy',
                     optimizer='adam',
                      metrics='accuracy')

binary_model_history = binary_model.fit(X_train, y_train,
                      validation_data=(X_val,y_val),
                      batch_size=100, epochs=4)

y_prob = binary_model.predict(X_test)
y_pred = (y_prob > 0.5).astype(int)

recall = recall_score(y_test, y_pred, average='weighted')
precision = precision_score(y_test, y_pred, average='weighted')
weighted_accuracy = accuracy_score(y_test, y_pred)
auc = roc_auc_score(y_test, y_prob)

print(f"Recall: {recall:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Weighted Accuracy: {weighted_accuracy:.2f}")
print(f"AUC: {auc:.2f}")

acc = binary_model_history.history['accuracy']
val_acc = binary_model_history.history['val_accuracy']
epochs = range(1,len(acc)+1)

plt.title('Accuracy')
plt.plot(epochs,acc,label='Train')
plt.plot(epochs,val_acc,color='red',label='Validation')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

loss_function = binary_model_history.history['loss']
val_loss_function = binary_model_history.history['val_loss']
epochs = range(1,len(loss_function)+1)

plt.title('Loss')
plt.plot(epochs,loss_function,label='Train')
plt.plot(epochs,val_loss_function,color='red',label='Validation')
plt.xlabel('Epochs')
plt.ylabel('Loss function')
plt.legend()
plt.show()

"""# Многоклассовый классификатор"""

data2 = pd.read_csv(file2_name)
data2.head(10)

data2.describe()

data2.info()

X = data2.drop('fetal_health', axis=1)
y = data2['fetal_health']

"""## Split data to train, test, and validation"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)

X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify=y_train, test_size = 0.2, random_state = 0)

"""## Normalize data"""

ss_train = StandardScaler()
X_train = ss_train.fit_transform(X_train)

ss_test = StandardScaler()
X_test = ss_train.fit_transform(X_test)

print(f'Размер исходного набора: X: {X.shape}; y: {y.shape}')
print(f'Размер обучающего набора: X: {X_train.shape}; y: {y_train.shape}')
print(f'Размер тестового набора: X: {X_test.shape}; y: {y_test.shape}')

"""## Support Vector Machine"""

model_svm = SVC(kernel='linear', C=1.0, probability = True, decision_function_shape='ovr')
model_svm.fit(X_train, y_train)

y_prediction = model_svm.predict(X_test)

"""### Recall, Precision, Weighted accuracy"""

recall = recall_score(y_test, y_prediction, average='weighted')
precision = precision_score(y_test, y_prediction, average='weighted')
weighted_accuracy = accuracy_score(y_test, y_prediction)

print(f"Recall: {recall:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Weighted Accuracy: {weighted_accuracy:.2f}")

"""### AUC Score"""

label_binarizer = LabelBinarizer().fit(y_train)
y_onehot_test = label_binarizer.transform(y_test)
y_onehot_test.shape

label_encoder = LabelEncoder()
y_prediction_encoded = label_encoder.fit_transform(y_prediction)

"""#### AUC for Normal (1)"""

label_binarizer.transform([1])

health_of_interest = 1
health_id = np.flatnonzero(label_binarizer.classes_ == health_of_interest)[0]
health_id

auc_score_1 = roc_auc_score(y_onehot_test[:, health_id], y_prediction_encoded)

print(f"AUC for Normal (1): {auc_score_1:.2f}")

"""#### AUC for Suspect (2)"""

label_binarizer.transform([2])

health_of_interest = 2
health_id = np.flatnonzero(label_binarizer.classes_ == health_of_interest)[0]
health_id

auc_score_2 = roc_auc_score(y_onehot_test[:, health_id], y_prediction_encoded)

print(f"AUC for Suspect (2): {auc_score_2:.2f}")

"""#### AUC for Pathological (3)"""

label_binarizer.transform([3])

health_of_interest = 3
health_id = np.flatnonzero(label_binarizer.classes_ == health_of_interest)[0]
health_id

auc_score_3 = roc_auc_score(y_onehot_test[:, health_id], y_prediction_encoded)

print(f"AUC for Pathological (3): {auc_score_3:.2f}")

"""#### Summary AUC score"""

print(f"AUC for Normal (1): {auc_score_1:.2f}")
print(f"AUC for Suspect (2): {auc_score_2:.2f}")
print(f"AUC for Pathological (3): {auc_score_3:.2f}")

"""## Random Forest Classifier"""

model_rf = RandomForestClassifier(n_estimators=100, random_state=42)
model_rf.fit(X_train, y_train)

y_prediction = model_rf.predict(X_test)

"""### Recall, Precission, Weighted Accuracy"""

recall = recall_score(y_test, y_prediction, average='weighted')
precision = precision_score(y_test, y_prediction, average='weighted')
weighted_accuracy = accuracy_score(y_test, y_prediction)

print(f"Recall: {recall:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Weighted Accuracy: {weighted_accuracy:.2f}")

"""### AUC ROC Score"""

label_binarizer = LabelBinarizer().fit(y_train)
y_onehot_test = label_binarizer.transform(y_test)
y_onehot_test.shape

label_encoder = LabelEncoder()
y_prediction_encoded = label_encoder.fit_transform(y_prediction)

"""#### AUC ROC for Normal (1)

##### AUC
"""

label_binarizer.transform([1])

health_of_interest = 1
health_id = np.flatnonzero(label_binarizer.classes_ == health_of_interest)[0]
health_id

auc_score_1 = roc_auc_score(y_onehot_test[:, health_id], y_prediction_encoded)

print(f"AUC for Normal (1): {auc_score_1:.2f}")

"""##### ROC"""

roc_display_1 = RocCurveDisplay.from_predictions(
    y_onehot_test[:, health_id],
    y_prediction_encoded,
    name=f"{health_of_interest} vs the rest",
    color="darkorange",
    plot_chance_level=True,
)

plt.axis("square")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("One-vs-Rest ROC curves:\n1 vs (2 & 3)")
plt.legend()
plt.show()

"""#### AUC ROC for Suspect (2)

##### AUC
"""

label_binarizer.transform([2])

health_of_interest = 2
health_id = np.flatnonzero(label_binarizer.classes_ == health_of_interest)[0]
health_id

auc_score_2 = roc_auc_score(y_onehot_test[:, health_id], y_prediction_encoded)

print(f"AUC for Suspect (2): {auc_score_2:.2f}")

"""##### ROC"""

roc_display_2 = RocCurveDisplay.from_predictions(
    y_onehot_test[:, health_id],
    y_prediction_encoded,
    name=f"{health_of_interest} vs the rest",
    color="darkorange",
    plot_chance_level=True,
)

plt.axis("square")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("One-vs-Rest ROC curves:\n2 vs (1 & 3)")
plt.legend()
plt.show()

"""#### AUC ROC for Pathological (3)

##### AUC
"""

label_binarizer.transform([3])

health_of_interest = 3
health_id = np.flatnonzero(label_binarizer.classes_ == health_of_interest)[0]
health_id

auc_score_3 = roc_auc_score(y_onehot_test[:, health_id], y_prediction_encoded)

print(f"AUC for Pathological (3): {auc_score_3:.2f}")

"""##### ROC"""

roc_display_3 = RocCurveDisplay.from_predictions(
    y_onehot_test[:, health_id],
    y_prediction_encoded,
    name=f"{health_of_interest} vs the rest",
    color="darkorange",
    plot_chance_level=True,
)

plt.axis("square")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("One-vs-Rest ROC curves:\n3 vs (1 & 2)")
plt.legend()
plt.show()

"""#### Summary AUC score"""

print(f"AUC for Normal (1): {auc_score_1:.2f}")
print(f"AUC for Suspect (2): {auc_score_2:.2f}")
print(f"AUC for Pathological (3): {auc_score_3:.2f}")

"""## Decision Tree Classifier"""

model_dt = DecisionTreeClassifier(max_depth=6)
model_dt.fit(X_train,y_train)

y_prediction = model_dt.predict(X_test)

"""### Recall, Precision, Weighted Accuracy"""

recall = recall_score(y_test, y_prediction, average='weighted')
precision = precision_score(y_test, y_prediction, average='weighted')
weighted_accuracy = accuracy_score(y_test, y_prediction)

print(f"Recall: {recall:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Weighted Accuracy: {weighted_accuracy:.2f}")

"""### AUC Score"""

label_binarizer = LabelBinarizer().fit(y_train)
y_onehot_test = label_binarizer.transform(y_test)
y_onehot_test.shape

label_encoder = LabelEncoder()
y_prediction_encoded = label_encoder.fit_transform(y_prediction)

"""#### AUC for Normal (1)"""

label_binarizer.transform([1])

health_of_interest = 1
health_id = np.flatnonzero(label_binarizer.classes_ == health_of_interest)[0]
health_id

auc_score_1 = roc_auc_score(y_onehot_test[:, health_id], y_prediction_encoded)

print(f"AUC for Normal (1): {auc_score_1:.2f}")

"""#### AUC for Suspect (2)"""

label_binarizer.transform([2])

health_of_interest = 2
health_id = np.flatnonzero(label_binarizer.classes_ == health_of_interest)[0]
health_id

auc_score_2 = roc_auc_score(y_onehot_test[:, health_id], y_prediction_encoded)

print(f"AUC for Suspect (2): {auc_score_2:.2f}")

"""#### AUC for Pathological (3)"""

label_binarizer.transform([3])

health_of_interest = 3
health_id = np.flatnonzero(label_binarizer.classes_ == health_of_interest)[0]
health_id

auc_score_3 = roc_auc_score(y_onehot_test[:, health_id], y_prediction_encoded)

print(f"AUC for Pathological (3): {auc_score_3:.2f}")

"""#### Summary AUC Score"""

print(f"AUC for Normal (1): {auc_score_1:.2f}")
print(f"AUC for Suspect (2): {auc_score_2:.2f}")
print(f"AUC for Pathological (3): {auc_score_3:.2f}")

"""## TensorFlow"""

y_train_c = to_categorical(y_train)
y_val_c = to_categorical(y_val)
y_test_c = to_categorical(y_test)

X_train.shape

multi_class_model = tf.keras.Sequential([
    tf.keras.layers.Dense(11,activation='relu',input_dim=21),
    tf.keras.layers.Dense(4, activation='softmax')
])

multi_class_model.summary()

learning_rate = 0.001  # You can adjust the learning rate as needed
optimizer = tf.keras.optimizers.Adagrad(learning_rate=learning_rate)

multi_class_model.compile(loss='categorical_crossentropy',
                     optimizer=optimizer,
                      metrics='accuracy')

multi_class_model_history = multi_class_model.fit(X_train, y_train_c,
                      validation_data=(X_val,y_val_c),
                      epochs=90)

acc = multi_class_model_history.history['accuracy']
val_acc = multi_class_model_history.history['val_accuracy']
epochs = range(1,len(acc)+1)

plt.title('Accuracy')
plt.plot(epochs,acc,label='Train')
plt.plot(epochs,val_acc,color='red',label='Validation')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

loss_function = multi_class_model_history.history['loss']
val_loss_function = multi_class_model_history.history['val_loss']
epochs = range(1,len(loss_function)+1)

plt.title('Loss')
plt.plot(epochs,loss_function,label='Train')
plt.plot(epochs,val_loss_function,color='red',label='Validation')
plt.xlabel('Epochs')
plt.ylabel('Loss function')
plt.legend()
plt.show()

"""# Регрессор"""

data3 = pd.read_csv(file3_name)
data3.head(10).style

data3.describe().style

data3.info(verbose=True, show_counts=True)

"""## Convert all columns with type object to float/integer (numeric)"""

# If it's not possible to convert the value to numeric then set the value to NaN, then use dropna to drop all the row with nill value
data3_numeric = data3.apply(pd.to_numeric, errors='coerce').dropna()
data3_numeric.info(verbose=True, show_counts=True)

data3.shape

data3_numeric.shape

"""## Divide data into X and y
Target = TOTALBTU (Total usage, in thousand BTU, 2009)
"""

X = data3_numeric.drop(['TOTALBTU'], axis=1)
y = data3_numeric['TOTALBTU']

"""## Split data to train, test and validation data"""

X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.25, random_state=0)

X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.3, random_state = 0)

"""## Normalize Data"""

ss_train = StandardScaler()
X_train = ss_train.fit_transform(X_train)

ss_test = StandardScaler()
X_test = ss_test.fit_transform(X_test)

print(f'Размер исходного набора: {X.shape}')
print(f'Размер обучающего набора: {X_train.shape}')
print(f'Размер тестового набора: {X_val.shape}')

"""## TensorFlow"""

model_regressor = tf.keras.Sequential([
    tf.keras.layers.Dense(12,activation='relu',input_dim=X_train.shape[1]),
    tf.keras.layers.Dense(8,activation='relu'),
    tf.keras.layers.Dense(1)
])

model_regressor.summary()

model_regressor.compile(loss='mse',
                      optimizer='adam',
                      metrics='mae')

regressor_history = model_regressor.fit(X_train, y_train,
                      validation_data=(X_val,y_val), batch_size = 40, epochs=100)

"""## MSE, MAE, R2"""

y_pred = model_regressor.predict(X_test)

mse = mean_squared_error(y_test, y_pred)

mae = mean_absolute_error(y_test, y_pred)

r2 = r2_score(y_test, y_pred)

print(f"MSE: {mse:.2f}")
print(f"MAE: {mae:.2f}")
print(f"R-squared (R^2): {r2:.2f}")

mae = regressor_history.history['mae']
val_mae = regressor_history.history['val_mae']
epochs = range(1,len(mae)+1)

plt.title('Mean Absolute Error')
plt.plot(epochs,mae,label='Train')
plt.plot(epochs,val_mae,color='red',label='Validation')
plt.xlabel('Epochs')
plt.ylabel('MAE')
plt.legend()
plt.show()

loss_function = regressor_history.history['loss']
val_loss_function = regressor_history.history['val_loss']
epochs = range(1,len(loss_function)+1)

plt.title('Loss')
plt.plot(epochs,loss_function,label='Train')
plt.plot(epochs,val_loss_function,color='red',label='Validation')
plt.xlabel('Epochs')
plt.ylabel('Loss function')
plt.legend()
plt.show()